# Viam Gemini Vision Service

This module provides a custom Viam Vision service that leverages Google's Gemini LLM to generate natural language descriptions of camera frames in real-time.

## Overview

The Gemini Vision service integrates Google's powerful Gemini multimodal models with Viam's robotics platform. This allows your robot to "understand" what it sees through rich natural language descriptions generated by Gemini.

**Core Features:**
- Capture images from any Viam-compatible camera
- Process images through Google's Gemini models
- Return concise natural language descriptions of visual content
- Implements standard Viam Vision service interfaces

## Prerequisites

- A Viam account with a registered machine
- A Google AI Studio account with API access
- A Gemini API key

## Configuration

When configuring this module in the Viam app, you'll need to specify the following attributes:

| Parameter | Description | Example |
|-----------|-------------|---------|
| `api_key` | Your Google Gemini API key | `YOUR_API_KEY` |
| `camera_name` | Resource name of the camera to use | `my-camera` |
| `model` | Gemini model to use | `gemini-2.0-flash` |
| `prompt` | Text prompt to send with each image | `"Describe what you see in this image"` |

### Example Configuration

```json
{
  "api_key": "YOUR_GEMINI_API_KEY",
  "camera_name": "my-camera",
  "model": "gemini-2.0-flash",
  "prompt": "Describe this image in detail, focusing on any objects or people present."
}
```

## Usage

Once configured, you can use the service through the Viam SDK:

```python
from viam.services.vision import VisionClient
from viam.rpc.dial import Credentials, DialOptions
from viam.robot.client import RobotClient

# Connect to your robot
robot = await RobotClient.at_address(
    "ROBOT_ADDRESS",
    Credentials(type="robot", payload="ROBOT_API_KEY"),
)

# Get the vision service
vision = VisionClient.from_robot(robot, "my-gemini-vision")

# Get an image description
result = await vision.capture_all_from_camera(
    "my-camera",
    return_classifications=True
)

# Print the classification (description)
for classification in result.classifications:
    print(f"Description: {classification.class_name}")
```

**Returns:**
- String containing the Gemini-generated description

## Example Use Cases

- **Visual alerts**: Generate notifications when specific objects or conditions are detected
- **Data collection**: Log descriptions of environments for later analysis
- **Accessibility**: Convert visual information into text for audio output
- **Autonomous decision-making**: Use scene descriptions to inform robot behaviors

## Limitations

- Image processing happens remotely through Google's API, requiring internet connectivity
- Response times depend on network conditions and Google API response times
- Gemini models may have their own limitations in accurately describing certain scenes

## Extending the Module

Future extensions could include:
- Support for object detection (currently stubbed)
- Filtering or categorization of detected objects
